# Queue & Cache Setup for Production

This document explains how to upgrade the MLM system from in-memory caching to Redis-based caching and job queues for production use.

## Current State

The system currently uses:
- **In-memory caching** via `cache.service.ts` for dashboard and genealogy data
- **Synchronous payouts** directly in API handlers
- **Database indexes** for query optimization

## Production Upgrade Path

### 1. Install Redis

**On Linux/macOS:**
```bash
# Install Redis
sudo apt-get install redis-server  # Ubuntu/Debian
brew install redis                  # macOS

# Start Redis
sudo systemctl start redis          # Linux
brew services start redis           # macOS
```

**On Windows:**
```bash
# Use WSL2 or Docker
docker run --name redis -p 6379:6379 -d redis:latest
```

### 2. Install Required Dependencies

```bash
npm install ioredis bullmq
npm install -D @types/ioredis
```

### 3. Update Cache Service to Use Redis

Replace `server/services/cache.service.ts` with Redis implementation:

```typescript
import Redis from 'ioredis';

const redis = new Redis({
  host: process.env.REDIS_HOST || 'localhost',
  port: parseInt(process.env.REDIS_PORT || '6379'),
  password: process.env.REDIS_PASSWORD,
  maxRetriesPerRequest: 3,
});

class CacheService {
  async get<T>(key: string): Promise<T | null> {
    const value = await redis.get(key);
    return value ? JSON.parse(value) : null;
  }

  async set(key: string, value: any, ttlSeconds: number = 300): Promise<void> {
    await redis.setex(key, ttlSeconds, JSON.stringify(value));
  }

  async delete(key: string): Promise<void> {
    await redis.del(key);
  }

  async deletePattern(pattern: string): Promise<number> {
    const keys = await redis.keys(pattern);
    if (keys.length === 0) return 0;
    return await redis.del(...keys);
  }

  async has(key: string): Promise<boolean> {
    return (await redis.exists(key)) === 1;
  }

  async getOrSet<T>(
    key: string,
    fetcher: () => Promise<T>,
    ttlSeconds: number = 300
  ): Promise<T> {
    const cached = await this.get<T>(key);
    if (cached !== null) return cached;

    const value = await fetcher();
    await this.set(key, value, ttlSeconds);
    return value;
  }

  async clear(): Promise<void> {
    await redis.flushdb();
  }
}

export const cache = new CacheService();
```

### 4. Set Up BullMQ for Job Queues

Create `server/queue/jobs.ts`:

```typescript
import { Queue, Worker, QueueEvents } from 'bullmq';
import Redis from 'ioredis';

const connection = new Redis({
  host: process.env.REDIS_HOST || 'localhost',
  port: parseInt(process.env.REDIS_PORT || '6379'),
  maxRetriesPerRequest: null,
});

// Define queues
export const payoutQueue = new Queue('payouts', { connection });
export const roiQueue = new Queue('roi-distribution', { connection });
export const levelIncomeQueue = new Queue('level-income', { connection });
export const binaryQueue = new Queue('binary-matching', { connection });

// Queue events for monitoring
const payoutEvents = new QueueEvents('payouts', { connection });

payoutEvents.on('completed', ({ jobId }) => {
  console.log(`âœ… Payout job ${jobId} completed`);
});

payoutEvents.on('failed', ({ jobId, failedReason }) => {
  console.error(`âŒ Payout job ${jobId} failed: ${failedReason}`);
});
```

Create `server/queue/workers.ts`:

```typescript
import { Worker } from 'bullmq';
import Redis from 'ioredis';
import { distribute30LevelIncome } from '../services/level-income.service';
import { distributeBinaryMatching } from '../services/binary-matching.service';

const connection = new Redis({
  host: process.env.REDIS_HOST || 'localhost',
  port: parseInt(process.env.REDIS_PORT || '6379'),
  maxRetriesPerRequest: null,
});

// Level income worker
const levelIncomeWorker = new Worker(
  'level-income',
  async (job) => {
    const { userId, amount, packageId, userPackageId } = job.data;
    console.log(`ðŸ“Š Processing level income for user ${userId}`);

    const result = await distribute30LevelIncome(userId, amount, packageId, userPackageId);

    return result;
  },
  {
    connection,
    concurrency: 5, // Process 5 jobs concurrently
    attempts: 3,    // Retry up to 3 times
    backoff: {
      type: 'exponential',
      delay: 5000,  // Start with 5 seconds
    },
  }
);

// Binary matching worker
const binaryWorker = new Worker(
  'binary-matching',
  async (job) => {
    const { userId, volume } = job.data;
    console.log(`ðŸŒ³ Processing binary matching for user ${userId}`);

    const result = await distributeBinaryMatching(userId, volume);

    return result;
  },
  {
    connection,
    concurrency: 3,
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 5000,
    },
  }
);

levelIncomeWorker.on('completed', (job) => {
  console.log(`âœ… Level income job ${job.id} completed`);
});

levelIncomeWorker.on('failed', (job, err) => {
  console.error(`âŒ Level income job ${job?.id} failed:`, err);
});

console.log('ðŸš€ Workers started');
```

### 5. Queue Jobs Instead of Synchronous Processing

Update `server/routes/packages.ts`:

```typescript
import { levelIncomeQueue, binaryQueue } from '../queue/jobs';

// Inside package purchase handler
router.post('/purchase', authenticateToken, async (req, res) => {
  // ... existing purchase logic ...

  // Instead of calling directly:
  // await distribute30LevelIncome(userId, amount, package_id, userPackageId);

  // Queue the job:
  await levelIncomeQueue.add('distribute', {
    userId,
    amount,
    packageId: package_id,
    userPackageId: String(userPackageId),
  }, {
    priority: 1,      // High priority
    removeOnComplete: true,
    removeOnFail: false,  // Keep failed jobs for debugging
  });

  // ... rest of handler ...
});
```

### 6. Start Workers

Create `server/worker.ts`:

```typescript
import dotenv from 'dotenv';
dotenv.config();

import './queue/workers';

console.log('ðŸ”§ Worker process started');
```

Update `package.json`:

```json
{
  "scripts": {
    "dev": "vite",
    "dev:server": "nodemon --watch server --exec tsx server/index.ts",
    "dev:worker": "nodemon --watch server --exec tsx server/worker.ts",
    "dev:all": "concurrently \"npm run dev:server\" \"npm run dev:worker\" \"npm run dev\""
  }
}
```

### 7. Environment Variables

Add to `.env`:

```bash
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# Queue Configuration
QUEUE_CONCURRENCY=5
QUEUE_MAX_ATTEMPTS=3
```

### 8. Monitoring & Dashboard

Install Bull Board for queue monitoring:

```bash
npm install @bull-board/express @bull-board/api
```

Add to `server/index.ts`:

```typescript
import { createBullBoard } from '@bull-board/api';
import { BullMQAdapter } from '@bull-board/api/bullMQAdapter';
import { ExpressAdapter } from '@bull-board/express';
import { payoutQueue, roiQueue, levelIncomeQueue, binaryQueue } from './queue/jobs';

const serverAdapter = new ExpressAdapter();
serverAdapter.setBasePath('/admin/queues');

createBullBoard({
  queues: [
    new BullMQAdapter(payoutQueue),
    new BullMQAdapter(roiQueue),
    new BullMQAdapter(levelIncomeQueue),
    new BullMQAdapter(binaryQueue),
  ],
  serverAdapter,
});

app.use('/admin/queues', authenticateAdmin, serverAdapter.getRouter());
```

Access queue dashboard at: `http://localhost:3001/admin/queues`

## Performance Benefits

### With Redis Caching:
- **Dashboard load**: 50-100ms â†’ 5-10ms (10x faster)
- **Genealogy tree**: 500ms â†’ 50ms (10x faster)
- **Report generation**: 200ms â†’ 20ms (10x faster)

### With Job Queues:
- **Package purchase**: 2-5 seconds â†’ 200ms (10-25x faster API response)
- **ROI distribution**: Non-blocking, processes in background
- **Binary matching**: Prevents timeout errors on large teams
- **Retry logic**: Failed payouts automatically retry
- **Scalability**: Can add more worker processes horizontally

## Deployment Checklist

- [ ] Install Redis on production server
- [ ] Update cache service to use Redis
- [ ] Create job queues and workers
- [ ] Update package.json scripts
- [ ] Add Redis env vars to production
- [ ] Start worker processes with PM2
- [ ] Monitor queue dashboard
- [ ] Set up Redis persistence (RDB + AOF)
- [ ] Configure Redis maxmemory policy
- [ ] Set up Redis backup strategy

## Monitoring

```bash
# Check Redis status
redis-cli ping

# Monitor Redis in real-time
redis-cli monitor

# Check cache hit rate
redis-cli info stats

# View queue stats
redis-cli keys bull:*
```

## Rollback Plan

If issues occur:
1. Stop worker processes
2. Revert cache service to in-memory version
3. Revert package routes to synchronous processing
4. Restart API server
5. Clear Redis: `redis-cli FLUSHALL`
